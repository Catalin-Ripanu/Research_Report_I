# -*- coding: utf-8 -*-
"""QFL_paper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u_xzWfbdpPrmww_BCtdOiKoIietnA7Ps

The main code implementing the QFL framework
"""


"""### Setup

Install TensorFlow Federated
"""


import nest_asyncio
nest_asyncio.apply()
#The next library is necessary to import the federated data
import h5py
import collections
import numpy as np
import tensorflow as tf
import tensorflow_federated as tff
import tensorflow_quantum as tfq
import cirq
import sympy
import os
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score
import pandas as pd
from datetime import datetime
from typing import Dict, List, Tuple, Any
import json
import yaml
from dataclasses import dataclass
import logging
import traceback
import argparse

import warnings
warnings.filterwarnings("ignore", category=RuntimeWarning)

tf.compat.v1.enable_v2_behavior()
np.random.seed(0)

from tensorflow_federated.python.simulation import ClientData

import os
import re
import json
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple
from collections import defaultdict

class ResultAnalyzer:
    def __init__(self, base_experiment_dir: str):
        self.base_dir = base_experiment_dir
        self._validate_directory()

    def _safe_mean(self, arr):
        """Handle empty arrays and NaN values"""
        arr = np.array(arr)
        if arr.size == 0:
            return 0.0
        return np.nanmean(arr)
    
    def _safe_std(self, arr):
        """Handle empty arrays and NaN values"""
        arr = np.array(arr)
        if arr.size == 0:
            return 0.0
        return np.nanstd(arr)

    def _validate_directory(self):
        if not os.path.exists(self.base_dir):
            raise ValueError(f"Experiment directory {self.base_dir} does not exist")

    def _load_metrics(self, experiment_path: str) -> Dict:
        """Load metrics from JSON file"""
        metrics_file = os.path.join(experiment_path, 'metrics', 'all_metrics.json')
        with open(metrics_file, 'r') as f:
            return json.load(f)

    def _parse_dataset_name(self, dataset_name: str) -> Tuple[int, int]:
        """Extract client count and sample size from dataset name"""
        # Handle both "30clients_160" and "clients30_160sample" patterns
        client_match = re.search(r'(?:clients(\d+)|(\d+)clients)', dataset_name)
        sample_match = re.search(r'(?:_(\d+)(?:sample)?|(\d+)sample)', dataset_name)
        
        clients = int(client_match.group(1) or client_match.group(2)) if client_match else 0
        if sample_match:
            samples = int(sample_match.group(1) or sample_match.group(2))
        else:
            samples = 0
        return clients, samples
    

    def plot_train_loss_evolution(self):
        """Plot training loss evolution for different client configurations"""
        plt.figure(figsize=(12, 7))
        
        # Specify the client configurations we want to plot
        target_clients = [1, 6, 12, 18, 24, 30, 60]
        
        # Define vibrant colors for different client configurations
        colors = {
            1: '#FF0000',    # Pure Red
            6: '#0000FF',    # Pure Blue
            12: '#00FF00',   # Pure Green
            18: '#FF00FF',   # Magenta
            24: '#FFD700',   # Gold
            30: '#00FFFF',   # Cyan
            60: '#FF8C00'    # Dark Orange
        }
        
        # Process all directories
        for exp_dir in os.listdir(self.base_dir):
            # Skip directories with 'noniid' in the name
            if 'noniid' in exp_dir or ('_12' in exp_dir and '30' in exp_dir):
                continue
                
            clients, samples = self._parse_dataset_name(exp_dir)
            
            # Check if this is a configuration we want to plot
            if clients in target_clients and samples == 160:
                try:
                    metrics = self._load_metrics(os.path.join(self.base_dir, exp_dir))
                    
                    # Extract training loss evolution
                    train_loss = [m['train_loss'] for m in metrics]
                    epochs = list(range(1, len(train_loss) + 1))
                    
                    # Plot training loss with vibrant color and markers
                    plt.plot(epochs, train_loss,
                            marker='o',
                            markersize=6,
                            markevery=5,
                            linestyle='-',
                            label=f'{clients} clients',
                            color=colors[clients],
                            linewidth=2)
                    
                except Exception as e:
                    print(f"Error processing directory {exp_dir}: {str(e)}")
                    continue
        
        plt.xlabel('Epoch')
        plt.ylabel('Training Loss')
        plt.title('Training Loss Evolution (160 samples/client)')
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        
        plt.savefig(os.path.join(self.base_dir, 'train_loss_evolution.png'),
                    bbox_inches='tight',
                    dpi=300)
        plt.close()

    def plot_valid_loss_evolution(self):
        """Plot validation loss evolution for different client configurations"""
        plt.figure(figsize=(12, 7))
        
        # Specify the client configurations we want to plot
        target_clients = [1, 6, 12, 18, 24, 30, 60]
        
        # Define vibrant colors for different client configurations
        colors = {
            1: '#FF0000',    # Pure Red
            6: '#0000FF',    # Pure Blue
            12: '#00FF00',   # Pure Green
            18: '#FF00FF',   # Magenta
            24: '#FFD700',   # Gold
            30: '#00FFFF',   # Cyan
            60: '#FF8C00'    # Dark Orange
        }
        
        # Process all directories
        for exp_dir in os.listdir(self.base_dir):
            # Skip directories with 'noniid' in the name
            if 'noniid' in exp_dir or ('_12' in exp_dir and '30' in exp_dir):
                continue
                
            clients, samples = self._parse_dataset_name(exp_dir)
            
            # Check if this is a configuration we want to plot
            if clients in target_clients and samples == 160:
                try:
                    metrics = self._load_metrics(os.path.join(self.base_dir, exp_dir))
                    
                    # Extract validation loss evolution
                    valid_loss = [m['test_loss'] for m in metrics]  # Assuming test_loss is validation loss
                    epochs = list(range(1, len(valid_loss) + 1))
                    
                    # Plot validation loss with vibrant color and markers
                    plt.plot(epochs, valid_loss,
                            marker='o',
                            markersize=6,
                            markevery=5,
                            linestyle='-',
                            label=f'{clients} clients',
                            color=colors[clients],
                            linewidth=2)
                    
                except Exception as e:
                    print(f"Error processing directory {exp_dir}: {str(e)}")
                    continue
        
        plt.xlabel('Epoch')
        plt.ylabel('Validation Loss')
        plt.title('Validation Loss Evolution (160 samples/client)')
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        
        plt.savefig(os.path.join(self.base_dir, 'valid_loss_evolution.png'),
                    bbox_inches='tight',
                    dpi=300)
        plt.close()

    def plot_client_accuracy_comparison(self):
        """Plot final training and testing accuracies versus number of clients"""
        # Store results for each client count
        results = {}
        
        # Process all directories
        for exp_dir in os.listdir(self.base_dir):
            if 'noniid' in exp_dir or ('_12' in exp_dir and '30' in exp_dir):
                continue
                
            clients, samples = self._parse_dataset_name(exp_dir)
            
            # Only process directories with 160 samples
            if clients <= 60 and samples == 160:
                try:
                    metrics = self._load_metrics(os.path.join(self.base_dir, exp_dir))
                    final_metrics = metrics[-1]  # Get metrics from last epoch
                    
                    results[clients] = {
                        'train': final_metrics['train_accuracy'],
                        'test': final_metrics['test_accuracy']
                    }
                    
                except Exception as e:
                    print(f"Error processing directory {exp_dir}: {str(e)}")
                    continue
        
        # Prepare data for plotting
        client_counts = sorted(results.keys())
        train_acc = [results[c]['train'] for c in client_counts]
        test_acc = [results[c]['test'] for c in client_counts]
        
        # Create the plot
        plt.figure(figsize=(10, 6))
        
        # Plot both training and testing accuracies
        plt.plot(client_counts, train_acc, 'bo-', label='Training', linewidth=2, markersize=8)
        plt.plot(client_counts, test_acc, 'rs-', label='Testing', linewidth=2, markersize=8)
        
        plt.xlabel('Number of Clients')
        plt.ylabel('Accuracy')
        plt.title('Training and Testing Accuracy vs Number of Clients (160 samples/client)')
        plt.grid(True)
        plt.legend()
        
        # Set axis limits
        plt.ylim(0.888, 1.0)
        plt.xticks(client_counts)
        
        plt.savefig(os.path.join(self.base_dir, 'client_accuracy_comparison.png'), 
                    bbox_inches='tight',
                    dpi=300)
        plt.close()

    def plot_sample_size_comparison(self):
        """Compare 30-client vs centralized performance across sample sizes"""
        sample_sizes = [80, 96, 120, 160, 248, 400]
        results = defaultdict(dict)
        
        # Process all directories
        for exp_dir in os.listdir(self.base_dir):
            try:
                # Skip non-iid directories
                if 'noniid' in exp_dir or ('_12' in exp_dir and '30' in exp_dir):
                    continue
                    
                clients, samples = self._parse_dataset_name(exp_dir)
                
                # We're only interested in centralized (1 client) and 30-client setups
                if clients == 30 and samples in sample_sizes:
                    metrics = self._load_metrics(os.path.join(self.base_dir, exp_dir))
                    final_acc = metrics[-1]['test_accuracy']
                    results[samples]['federated'] = final_acc
                    print(f"Found federated result for {samples} samples: {final_acc}")
                
                # Handle centralized case (look for directories with "clients1" or "1clients")
                elif clients == 1 and samples in sample_sizes:
                    metrics = self._load_metrics(os.path.join(self.base_dir, exp_dir))
                    final_acc = metrics[-1]['test_accuracy']
                    results[samples]['centralized'] = final_acc
                    print(f"Found centralized result for {samples} samples: {final_acc}")
                    
            except Exception as e:
                print(f"Error processing directory {exp_dir}: {str(e)}")
                continue

        # Prepare data for plotting
        sample_sizes_found = sorted(results.keys())
        
        if not sample_sizes_found:
            print("No valid data found for sample size comparison")
            return
            
        centralized_acc = []
        federated_acc = []
        valid_sample_sizes = []
        
        # Only plot points where we have both centralized and federated results
        for size in sample_sizes_found:
            if 'centralized' in results[size] and 'federated' in results[size]:
                centralized_acc.append(results[size]['centralized'])
                federated_acc.append(results[size]['federated'])
                valid_sample_sizes.append(size)
        
        print(f"Plotting points:")
        print(f"Sample sizes: {valid_sample_sizes}")
        print(f"Centralized accuracies: {centralized_acc}")
        print(f"Federated accuracies: {federated_acc}")
        
        plt.figure(figsize=(10, 6))
        
        # Plot with markers at each point
        plt.plot(valid_sample_sizes, centralized_acc, 'rs--', 
                label='Centralized (1 client)', linewidth=2, markersize=8)
        plt.plot(valid_sample_sizes, federated_acc, 'bd-', 
                label='Federated (30 clients)', linewidth=2, markersize=8)
    
        
        plt.xlabel('Samples per Client')
        plt.ylabel('Testing Accuracy')
        plt.title('Centralized vs Federated Performance by Sample Size')
        plt.legend()
        plt.grid(True)
        
        # Set y-axis limits for better visualization
        plt.ylim(0.89, 1.0)
        
        # Use actual sample sizes for x-axis ticks
        plt.xticks(valid_sample_sizes)
        
        plt.savefig(os.path.join(self.base_dir, 'sample_size_comparison.png'), 
                    bbox_inches='tight',
                    dpi=300)
        plt.close()

    def plot_optimizer_comparison(self):
        """Compare different optimizer configurations using line plots (test accuracy only)"""
        optimizer_configs = {
            'RMSprop': [0.002],
            'SGD': [0.02],
            'Adam': [0.002, 0.02, 0.2]
        }
        
        # Dictionary to store accuracy evolution for each configuration
        results = {}
        
        # Process all directories
        for exp_dir in os.listdir(self.base_dir):
            if 'noniid' in exp_dir:
                continue
                
            if '30' in exp_dir and '160' in exp_dir:
                config_path = os.path.join(self.base_dir, exp_dir, 'config', 'config.yaml')
                if os.path.exists(config_path):
                    with open(config_path, 'r') as f:
                        config = yaml.safe_load(f)
                        opt = config['client_optimizer']
                        lr = config['client_learning_rate']
                        
                        if opt in optimizer_configs and lr in optimizer_configs[opt]:
                            # Get metrics for all epochs
                            metrics = self._load_metrics(os.path.join(self.base_dir, exp_dir))
                            
                            # Store only testing accuracy evolution
                            config_name = f"{opt} (lr={lr})"
                            if config_name not in results:
                                results[config_name] = [m['test_accuracy'] for m in metrics]
        
        # Create the plot
        plt.figure(figsize=(12, 6))
        
        # Define a color palette with stronger, more vibrant colors
        colors = [
            '#FF0000',  # Pure Red
            '#0000FF',  # Pure Blue
            '#00FF00',  # Pure Green
            '#FF00FF',  # Magenta
            '#FFD700',  # Gold
            '#00FFFF',  # Cyan
            '#FF8C00',  # Dark Orange
            '#8B008B',  # Dark Magenta
            '#006400'   # Dark Green
        ]
        
        # Different line styles for better distinction
        line_styles = ['-', '--', '-.', ':']
        markers = ['o', 's', '^', 'D', 'v', '>', '<', 'p']
        
        # Plot each configuration with unique style combinations
        for idx, (config_name, accuracies) in enumerate(results.items()):
            epochs = range(1, len(accuracies) + 1)
            
            # Cycle through styles if we have more configurations than colors
            color = colors[idx % len(colors)]
            line_style = line_styles[idx % len(line_styles)]
            marker = markers[idx % len(markers)]
            
            plt.plot(epochs, accuracies,
                    linestyle=line_style,
                    marker=marker,
                    label=config_name,
                    color=color,
                    linewidth=2,
                    markersize=6,
                    markevery=5)
        
        plt.xlabel('Epoch')
        plt.ylabel('Test Accuracy')
        plt.title('Optimizer Performance Comparison (30 clients, 160 samples)')
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        
        # Set y-axis limits for better visualization
        plt.ylim(0.35, 1.0)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.base_dir, 'optimizer_comparison.png'),
                    bbox_inches='tight',
                    dpi=300)
        plt.close()

    def plot_metric_variability(self):
        """Plot error bars for accuracy and loss metrics across multiple runs"""
        # Define metrics we want to analyze
        metric_pairs = [
            ('accuracy', 'Accuracy'), 
            ('loss', 'Loss')
        ]
        
        results = defaultdict(lambda: defaultdict(list))
        
        # Debug counter
        processed_dirs = 0
        
        # Collect data from multiple runs
        for exp_dir in os.listdir(self.base_dir):
            if '24clients_160' in exp_dir:
                try:
                    metrics = self._load_metrics(os.path.join(self.base_dir, exp_dir))
                    final_metrics = metrics[-1]  # Get metrics from last epoch
                    
                    # Collect both training and testing metrics independently
                    for base_metric, _ in metric_pairs:
                        train_key = f'train_{base_metric}'
                        test_key = f'test_{base_metric}'
                        
                        # Handle train and test separately
                        if train_key in final_metrics:
                            results['train'][base_metric].append(float(final_metrics[train_key]))
                        if test_key in final_metrics:
                            results['test'][base_metric].append(float(final_metrics[test_key]))
                    
                    processed_dirs += 1
                
                except Exception as e:
                    print(f"Error processing directory {exp_dir}: {str(e)}")
                    continue
        
        # Check if we have any data before proceeding
        if processed_dirs == 0:
            print("No valid data found for plotting")
            return
        
        # Prepare plot data
        x = np.arange(len(metric_pairs))
        width = 0.35  # Width of bars
        
        # Calculate means and standard deviations
        train_means, test_means = [], []
        train_errs, test_errs = [], []
        
        for metric, _ in metric_pairs:
            train_data = results['train'].get(metric, [])
            test_data = results['test'].get(metric, [])
            
            # Skip metrics with no data
            if not train_data or not test_data:
                print(f"Warning: No data for {metric}")
                continue
            
            train_means.append(np.mean(train_data))
            test_means.append(np.mean(test_data))
            
            # Use standard deviation for variability
            train_errs.append(np.std(train_data))
            test_errs.append(np.std(test_data))
        
        # If no valid metrics are left, exit
        if not train_means or not test_means:
            print("No valid metrics to plot")
            return
        
        # Calculate error percentages with safety checks
        train_err_pcts = []
        test_err_pcts = []
        
        for mean, err in zip(train_means, train_errs):
            pct = 100 * (err / mean) if mean != 0 else 0
            train_err_pcts.append(pct)
        
        for mean, err in zip(test_means, test_errs):
            pct = 100 * (err / mean) if mean != 0 else 0
            test_err_pcts.append(pct)
        
        # Create the plot
        plt.figure(figsize=(12, 6))
        
        # Create bars with vibrant colors
        train_bars = plt.bar(x - width/2, train_means, width, 
                            yerr=train_errs, 
                            label='Training',
                            color='#FF0000',  # Vibrant red
                            capsize=5)
        test_bars = plt.bar(x + width/2, test_means, width, 
                            yerr=test_errs, 
                            label='Testing',
                            color='#0000FF',  # Vibrant blue
                            capsize=5)
        
        # Add error percentage labels with safety check
        def add_error_labels(bars, error_pcts):
            for bar, pct in zip(bars, error_pcts):
                height = bar.get_height()
                if height > 0:  # Only add label if bar has positive height
                    plt.text(bar.get_x() + bar.get_width()/2., height,
                            f'±{pct:.1f}%',
                            ha='center', va='bottom')
        
        add_error_labels(train_bars, train_err_pcts)
        add_error_labels(test_bars, test_err_pcts)
        
        # Customize plot
        plt.xlabel('Metric')
        plt.ylabel('Score')
        plt.title('Performance Metric Variability\n(24 clients, 160 samples each)')
        plt.xticks(x, [label for _, label in metric_pairs])
        plt.legend()
        plt.grid(True, linestyle='--', alpha=0.7)
        
        # Adjust layout and save
        plt.tight_layout()
        plt.savefig(os.path.join(self.base_dir, 'metric_variability.png'),
                    bbox_inches='tight',
                    dpi=300)
        plt.close()


# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class ExperimentConfig:
    """Configuration for a single experiment"""
    num_rounds: int = 13
    client_learning_rate: float = 0.02
    server_learning_rate: float = 1.0
    train_split: float = 0.8
    client_optimizer: str = 'SGD'
    server_optimizer: str = 'SGD'
    random_seed: int = 0

class QuantumCircuitBuilder:
    """Handles quantum circuit construction"""
    
    @staticmethod
    def cluster_state_circuit(bits):
        circuit = cirq.Circuit()
        circuit.append(cirq.H.on_each(bits))
        for this_bit, next_bit in zip(bits, bits[1:] + [bits[0]]):
            circuit.append(cirq.CZ(this_bit, next_bit))
        return circuit

    @staticmethod
    def one_qubit_unitary(bit, symbols):
        """Create a single-qubit unitary circuit"""
        return cirq.Circuit(
            cirq.X(bit)**symbols[0],
            cirq.Y(bit)**symbols[1],
            cirq.Z(bit)**symbols[2])

    @staticmethod
    def two_qubit_unitary(bits, symbols):
        """Create a two-qubit unitary circuit"""
        circuit = cirq.Circuit()
        circuit += QuantumCircuitBuilder.one_qubit_unitary(bits[0], symbols[0:3])
        circuit += QuantumCircuitBuilder.one_qubit_unitary(bits[1], symbols[3:6])
        circuit += [cirq.ZZ(*bits)**symbols[6]]
        circuit += [cirq.YY(*bits)**symbols[7]]
        circuit += [cirq.XX(*bits)**symbols[8]]
        circuit += QuantumCircuitBuilder.one_qubit_unitary(bits[0], symbols[9:12])
        circuit += QuantumCircuitBuilder.one_qubit_unitary(bits[1], symbols[12:])
        return circuit

    @staticmethod
    def two_qubit_pool(source_qubit, sink_qubit, symbols):
        """Create a quantum pooling circuit"""
        pool_circuit = cirq.Circuit()
        sink_basis_selector = QuantumCircuitBuilder.one_qubit_unitary(sink_qubit, symbols[0:3])
        source_basis_selector = QuantumCircuitBuilder.one_qubit_unitary(source_qubit, symbols[3:6])
        pool_circuit.append(sink_basis_selector)
        pool_circuit.append(source_basis_selector)
        pool_circuit.append(cirq.CNOT(control=source_qubit, target=sink_qubit))
        pool_circuit.append(sink_basis_selector**-1)
        return pool_circuit

    @staticmethod
    def quantum_conv_circuit(bits, symbols):
        """Create a quantum convolution circuit"""
        circuit = cirq.Circuit()
        for first, second in zip(bits[0::2], bits[1::2]):
            circuit += QuantumCircuitBuilder.two_qubit_unitary([first, second], symbols)
        for first, second in zip(bits[1::2], bits[2::2] + [bits[0]]):
            circuit += QuantumCircuitBuilder.two_qubit_unitary([first, second], symbols)
        return circuit

    @staticmethod
    def quantum_pool_circuit(source_bits, sink_bits, symbols):
        """Create a quantum pooling layer"""
        circuit = cirq.Circuit()
        for source, sink in zip(source_bits, sink_bits):
            circuit += QuantumCircuitBuilder.two_qubit_pool(source, sink, symbols)
        return circuit

    @staticmethod
    def create_model_circuit(qubits):
        """Create the complete QCNN model circuit"""
        model_circuit = cirq.Circuit()
        symbols = sympy.symbols('qconv0:63')  # 64 learnable circuit parameters
        
        # First convolution and pooling layer
        model_circuit += QuantumCircuitBuilder.quantum_conv_circuit(qubits, symbols[0:15])
        model_circuit += QuantumCircuitBuilder.quantum_pool_circuit(qubits[:4], qubits[4:], symbols[15:21])
        
        # Second convolution and pooling layer
        model_circuit += QuantumCircuitBuilder.quantum_conv_circuit(qubits[4:], symbols[21:36])
        model_circuit += QuantumCircuitBuilder.quantum_pool_circuit(qubits[4:6], qubits[6:], symbols[36:42])
        
        # Third convolution and pooling layer
        model_circuit += QuantumCircuitBuilder.quantum_conv_circuit(qubits[6:], symbols[42:57])
        model_circuit += QuantumCircuitBuilder.quantum_pool_circuit([qubits[6]], [qubits[7]], symbols[57:63])
        
        return model_circuit

class MetricsManager:
    """Handles metrics calculation, storage, and visualization"""
    
    def __init__(self, base_dir: str):
        self.base_dir = base_dir
        self.metrics_history = {
            'train_loss': [], 'test_loss': [],
            'train_accuracy': [], 'test_accuracy': []
        }

    def _calculate_additional_metrics(self, accuracy):
        """Calculate F1, precision, recall from accuracy assuming balanced dataset"""
        # For binary classification with balanced dataset, these metrics converge
        return {
            'precision': float(accuracy),
            'recall': float(accuracy),
            'f1': float(accuracy)  # F1 is harmonic mean of precision and recall
        }

    def update_metrics(self, metrics_dict: Dict[str, float], epoch: int):
        """Update metrics history and save to disk"""
        for key, value in metrics_dict.items():
            if key in self.metrics_history:
                self.metrics_history[key].append(float(value))
        self._save_metrics(metrics_dict, epoch)

    def _save_metrics(self, metrics_dict: Dict[str, float], epoch: int):
        """Save metrics to JSON file"""
        # Convert all numpy values to Python native types
        metrics_with_epoch = {
            key: float(value) if isinstance(value, (np.floating, np.integer)) else value
            for key, value in metrics_dict.items()
        }
        metrics_with_epoch['epoch'] = epoch
        
        # Calculate additional metrics
        train_additional = self._calculate_additional_metrics(metrics_dict['train_accuracy'])
        test_additional = self._calculate_additional_metrics(metrics_dict['test_accuracy'])
        
        # Add additional metrics with train/test prefix
        for metric in ['precision', 'recall', 'f1']:
            metrics_with_epoch[f'train_{metric}'] = train_additional[metric]
            metrics_with_epoch[f'test_{metric}'] = test_additional[metric]
        
        # Save metrics to CSV (only basic metrics)
        metrics_file = os.path.join(self.base_dir, 'metrics', 'metrics.csv')
        df = pd.DataFrame([{
            'epoch': epoch,
            'train_loss': float(metrics_dict['train_loss']),
            'test_loss': float(metrics_dict['test_loss']),
            'train_accuracy': float(metrics_dict['train_accuracy']),
            'test_accuracy': float(metrics_dict['test_accuracy'])
        }])
        
        if os.path.exists(metrics_file):
            df.to_csv(metrics_file, mode='a', header=False, index=False)
        else:
            df.to_csv(metrics_file, index=False)
            
        # Save all metrics (including additional ones) to JSON
        json_file = os.path.join(self.base_dir, 'metrics', 'all_metrics.json')
        if os.path.exists(json_file):
            with open(json_file, 'r') as f:
                all_metrics = json.load(f)
        else:
            all_metrics = []
            
        all_metrics.append(metrics_with_epoch)
        with open(json_file, 'w') as f:
            json.dump(all_metrics, f, indent=2)

    def plot_all_metrics(self):
        """Generate and save plots for main metrics only"""
        metrics = ['loss', 'accuracy']
        for metric in metrics:
            if f'train_{metric}' in self.metrics_history:
                self._create_metric_plot(metric)

    def _create_metric_plot(self, metric: str):
        """Create and save a single metric plot"""
        plt.figure(figsize=(10, 6))
        plt.plot(self.metrics_history[f'train_{metric}'], label=f'Training {metric}')
        plt.plot(self.metrics_history[f'test_{metric}'], label=f'Testing {metric}')
        plt.xlabel('Epoch')
        plt.ylabel(metric.replace('_', ' ').title())
        plt.title(f'{metric.replace("_", " ").title()} vs Epoch')
        plt.legend()
        plt.grid(True)
        plt.savefig(os.path.join(self.base_dir, 'plots', f'{metric}_plot.png'))
        plt.close()

class ExperimentManager:
    """Manages the entire experiment lifecycle"""
    
    def __init__(self, config: ExperimentConfig):
        self.config = config
        np.random.seed(config.random_seed)
        tf.random.set_seed(config.random_seed)

    def setup_experiment(self, dir_name: str, dataset_path: str) -> Tuple[str, MetricsManager]:
        """Setup experiment directories and managers"""
        dataset_name = os.path.basename(dataset_path).replace('.hdf5', '')
        base_dir = self._create_experiment_folders(dir_name, dataset_name)
        self._save_config(base_dir)
        return base_dir, MetricsManager(base_dir)

    def _create_experiment_folders(self, dir_name: str, dataset_name: str) -> str:
        """Create experiment directory structure"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        base_dir = f'{dir_name}/{dataset_name}_{timestamp}'
        for folder in ['plots', 'metrics', 'config']:  # Removed 'models' directory
            os.makedirs(os.path.join(base_dir, folder), exist_ok=True)
        return base_dir

    def _save_config(self, base_dir: str):
        """Save experiment configuration"""
        config_path = os.path.join(base_dir, 'config', 'config.yaml')
        with open(config_path, 'w') as f:
            yaml.dump(self.config.__dict__, f)

class SignAccuracy(tf.keras.metrics.Metric):
    """Custom metric to compute accuracy based on sign agreement"""
    def __init__(self, name='sign_accuracy', **kwargs):
        super().__init__(name=name, **kwargs)
        self.correct = self.add_weight(name='correct', initializer='zeros')
        self.total = self.add_weight(name='total', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true_sign = tf.sign(y_true)
        y_pred_sign = tf.sign(y_pred)
        matches = tf.equal(y_true_sign, y_pred_sign)
        self.correct.assign_add(tf.reduce_sum(tf.cast(matches, tf.float32)))
        self.total.assign_add(tf.cast(tf.size(y_true), tf.float32))

    def result(self):
        return self.correct / self.total

    def reset_states(self):
        self.correct.assign(0.0)
        self.total.assign(0.0)

class QuantumFederatedLearner:
    """Main class for quantum federated learning"""
    
    def __init__(self, config: ExperimentConfig):
        self.config = config
        self.experiment_manager = ExperimentManager(config)
        self.true_labels = None
        self.predictions = None

    def _create_keras_model(self):
        """Create the Keras model with quantum layers"""
        cluster_state_bits = cirq.GridQubit.rect(1, 8)
        readout_operators = cirq.Z(cluster_state_bits[-1])

        excitation_input = tf.keras.Input(shape=(), dtype=tf.dtypes.string)
        cluster_state = tfq.layers.AddCircuit()(
            excitation_input, 
            prepend=QuantumCircuitBuilder.cluster_state_circuit(cluster_state_bits)
        )

        quantum_model = tfq.layers.PQC(
            QuantumCircuitBuilder.create_model_circuit(cluster_state_bits),
            readout_operators
        )(cluster_state)

        return tf.keras.Model(inputs=[excitation_input], outputs=[quantum_model])

    def _get_dataset_labels(self, dataset):
        """Extract true labels from a dataset"""
        try:
            labels = []
            for client_dataset in dataset:
                for x, y in client_dataset:
                    labels.extend(y.numpy().flatten())
            return np.array(labels)
        except Exception as e:
            logger.error(f"Error extracting labels: {str(e)}")
            raise

    def _get_model_predictions(self, state, dataset, evaluation):
        """Get model predictions for a dataset"""
        try:
            # Get predictions using TFF evaluation
            eval_metrics = evaluation(state.model, dataset)
            return (eval_metrics['binary_accuracy'] > 0.5).astype(int)
        except Exception as e:
            logger.error(f"Error getting predictions: {str(e)}")
            raise

    def _get_stable_input_spec(self, client_data: ClientData):
        example_dataset = client_data.create_tf_dataset_for_client(client_data.client_ids[0])
        return example_dataset.element_spec

    def _build_iterative_process(self, model_fn):
        """Build the federated averaging process"""
        return tff.learning.build_federated_averaging_process(
            model_fn=model_fn,
            client_optimizer_fn=self._get_optimizer(
                self.config.client_optimizer, 
                self.config.client_learning_rate
            ),
            server_optimizer_fn=self._get_optimizer(
                self.config.server_optimizer, 
                self.config.server_learning_rate
            )
        )

    @staticmethod
    def _get_optimizer(optimizer_name: str, learning_rate: float):
        """Get optimizer function based on name"""
        optimizers = {
            'Adam': tf.keras.optimizers.Adam,
            'SGD': tf.keras.optimizers.SGD,
            'RMSprop': tf.keras.optimizers.RMSprop
        }
        return lambda: optimizers[optimizer_name](learning_rate=learning_rate)

    @staticmethod
    def _calculate_metrics(train_metrics: Dict, test_metrics: Dict,
                         train_labels: np.ndarray, test_labels: np.ndarray,
                         train_preds: np.ndarray, test_preds: np.ndarray) -> Dict[str, float]:
        """Calculate comprehensive metrics including precision, recall, and F1"""
        metrics = {
            'train_loss': float(train_metrics['loss']),
            'test_loss': float(test_metrics['loss']),
            'train_accuracy': float(train_metrics['binary_accuracy']),
            'test_accuracy': float(test_metrics['binary_accuracy']),
            'train_precision': float(precision_score(train_labels, train_preds, zero_division=0)),
            'test_precision': float(precision_score(test_labels, test_preds, zero_division=0)),
            'train_recall': float(recall_score(train_labels, train_preds, zero_division=0)),
            'test_recall': float(recall_score(test_labels, test_preds, zero_division=0)),
            'train_f1': float(f1_score(train_labels, train_preds, zero_division=0)),
            'test_f1': float(f1_score(test_labels, test_preds, zero_division=0))
        }
        return metrics

    def _create_model_fn(self):
        """Create the quantum model function for TFF with corrected metrics"""
        def model_fn():
            keras_model = self._create_keras_model()
            return tff.learning.from_keras_model(
                keras_model=keras_model,
                input_spec=self.input_spec,
                loss=tf.keras.losses.MeanSquaredError(),
                metrics=[SignAccuracy()]  # Replace BinaryAccuracy with SignAccuracy
            )
        return model_fn

    def _prepare_data(self, dataset_path: str, num_clients: int) -> Tuple[List, List]:
        """Prepare federated datasets with corrected data processing"""
        try:
            dataset = tff.simulation.hdf5_client_data.HDF5ClientData(dataset_path)
            shuffled_ids = dataset.client_ids.copy()
            np.random.shuffle(shuffled_ids)
            
            if num_clients != 1:
                train_size = int(num_clients * self.config.train_split)
                train_ids = shuffled_ids[:train_size]
                test_ids = shuffled_ids[train_size:]
            else:
                train_size = 1
                train_ids = shuffled_ids[:train_size]
                test_ids = train_ids

            def preprocess_dataset(client_dataset):
                return client_dataset.map(
                    # Remove extra dimensions with tf.squeeze
                    lambda x: (tf.squeeze(x['x']), 2.0 * tf.squeeze(tf.cast(x['y'], tf.float32)) - 1.0)
                ).batch(1)

            train_data = [preprocess_dataset(dataset.create_tf_dataset_for_client(cid)) 
                        for cid in train_ids]
            test_data = [preprocess_dataset(dataset.create_tf_dataset_for_client(cid)) 
                        for cid in test_ids]

            # Set correct input specification (shape () for scalar elements)
            self.input_spec = (
                tf.TensorSpec(shape=(None,), dtype=tf.string),  # x: (batch_size,)
                tf.TensorSpec(shape=(None,), dtype=tf.float32)  # y: (batch_size,)
            )

            return train_data, test_data
        except Exception as e:
            logger.error(f"Error preparing data: {str(e)}")
            raise

    def _collect_predictions(self, model_weights, dataset):
        """Collect predictions with NaN handling"""
        all_preds, all_labels = [], []
        model = self._create_keras_model()
        model.set_weights(model_weights.trainable + model_weights.non_trainable)
        
        for client_data in dataset:
            for x, y in client_data:
                try:
                    pred = model(x)
                    # Replace NaN predictions with random values
                    pred = np.nan_to_num(pred.numpy(), nan=np.random.uniform(-1, 1))
                    all_preds.extend(pred.flatten())
                    all_labels.extend(y.numpy().flatten())
                except Exception as e:
                    logger.warning(f"Prediction error: {str(e)}")
                    continue
                    
        return np.array(all_preds), np.array(all_labels)

    def train_and_evaluate(self, dir_name: str, dataset_path: str, num_clients: int) -> Tuple[str, Dict[str, List[float]]]:
        """Training loop with corrected metric calculation"""
        base_dir, metrics_manager = self.experiment_manager.setup_experiment(dir_name, dataset_path)
        train_data, test_data = self._prepare_data(dataset_path, num_clients)
        
        model_fn = self._create_model_fn()
        iterative_process = self._build_iterative_process(model_fn)
        state = iterative_process.initialize()
        
        for round_num in range(1, self.config.num_rounds + 1):
            # Training step
            state, metrics = iterative_process.next(state, train_data)
            
            # Collect predictions and calculate metrics
            train_preds, train_labels = self._collect_predictions(state.model, train_data)
            test_preds, test_labels = self._collect_predictions(state.model, test_data)
            
            # Convert to binary labels for sklearn metrics
            train_labels_bin = (train_labels > 0).astype(int)
            test_labels_bin = (test_labels > 0).astype(int)
            train_preds_bin = (train_preds > 0).astype(int)
            test_preds_bin = (test_preds > 0).astype(int)
            
            # Calculate comprehensive metrics
            metrics_dict = {
                'train_loss': np.mean((train_preds - train_labels)**2),
                'test_loss': np.mean((test_preds - test_labels)**2),
                'train_accuracy': np.mean(np.sign(train_preds) == np.sign(train_labels)),
                'test_accuracy': np.mean(np.sign(test_preds) == np.sign(test_labels)),
                'train_precision': precision_score(train_labels_bin, train_preds_bin, zero_division=0),
                'test_precision': precision_score(test_labels_bin, test_preds_bin, zero_division=0),
                'train_recall': recall_score(train_labels_bin, train_preds_bin, zero_division=0),
                'test_recall': recall_score(test_labels_bin, test_preds_bin, zero_division=0),
                'train_f1': f1_score(train_labels_bin, train_preds_bin, zero_division=0),
                'test_f1': f1_score(test_labels_bin, test_preds_bin, zero_division=0)
            }
            
            metrics_manager.update_metrics(metrics_dict, round_num)
            logger.info(f"Round {round_num}/{self.config.num_rounds} - "
                       f"Train loss: {metrics_dict['train_loss']:.4f}, "
                       f"Test loss: {metrics_dict['test_loss']:.4f}, "
                       f"Train accuracy: {metrics_dict['train_accuracy']:.4f}, "
                       f"Test accuracy: {metrics_dict['test_accuracy']:.4f}")
            
        logger.info("Training completed. Generating final plots...")
        metrics_manager.plot_all_metrics()
        return base_dir, metrics_manager.metrics_history

def main():
    """Main entry point"""
    # Load configuration
    logger.info(f"TensorFlow version: {tf.__version__}")
    logger.info(f"TFF version: {tff.__version__}")
    arg_parser = argparse.ArgumentParser(prog="qfl_paper")
    arg_parser.add_argument("--experiment_dir", type=str, default="experiment", 
                          help="Base directory for all experiment results")
    args = arg_parser.parse_args()
    dir_name = args.experiment_dir
    config = ExperimentConfig()
    learner = QuantumFederatedLearner(config)
    
    # Process all datasets
    datasets_dir = './Datasets'
    all_experiments = []

    if False:
        for dataset_file in os.listdir(datasets_dir):
            if dataset_file.endswith('.hdf5') and "24clients" in dataset_file:
                logger.info(f"Processing dataset: {dataset_file}")
                dataset_path = os.path.join(datasets_dir, dataset_file)
                
                try:
                    # Client count extraction
                    filename_parts = re.findall(r'\d+', dataset_file)
                    num_clients = int(filename_parts[0]) if filename_parts else 0
                    
                    # Get actual client count from dataset
                    dataset = tff.simulation.hdf5_client_data.HDF5ClientData(dataset_path)
                    actual_clients = len(dataset.client_ids)
                    num_clients = min(num_clients, actual_clients) if num_clients > 0 else actual_clients
                    
                    if num_clients == 0:
                        logger.error(f"No valid clients in {dataset_file}")
                        continue
                    
                    # Run training and evaluation
                    base_dir, metrics = learner.train_and_evaluate(
                        dir_name=dir_name,
                        dataset_path=dataset_path,
                        num_clients=num_clients
                    )
                    all_experiments.append(base_dir)
                    logger.info(f"Results saved in: {base_dir}")
                    
                except Exception as e:
                    logger.error(f"Error processing {dataset_file}: {str(e)}\n{traceback.format_exc()}")
                    continue

    # Generate comparative analysis after all experiments
    logger.info("Generating comparative analysis graphs...")
    try:
        analyzer = ResultAnalyzer(dir_name)
        analyzer.plot_metric_variability()
        logger.info(f"Comparative analysis graphs saved in: {dir_name}")
    except Exception as e:
        logger.error(f"Failed to generate analysis graphs: {str(e)}")

if __name__ == "__main__":
    main()
